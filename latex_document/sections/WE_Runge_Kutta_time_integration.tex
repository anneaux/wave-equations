\section{Runge-Kutta time-integration}

After the former chapters treated numerical method to calculate the (spatial) derivative of a given function, we now want to devote to the temporal integration in order to deal with the initial value problem (IVP) of an ordinary differential equation (ODE). I.e. we want to find the function $u(t)$, and all we know is its derivative
\begin{equation}
  \frac{\mathrm{d} u}{\mathrm{d} t} = F(u)
  \label{eq:drvtv}
\end{equation}
and the initial value $u(t = 0) = u_0$.
We assume $F(u)$ to be continuous, especially Lipschitz continuous, to make sure that there exists exactly one solution for the IVP.
% QUELLE : 2011_Book_Numerik-Algorithmen
\subsection{General Principle of Numerical Integration Methods}
Similarly to the spatial differential methods, the first step is to discretize the respective interval. Generally, this discretization does not have to be equidistant, so we define
\begin{equation}
  \Delta t_i = t_{i+1} - t_i >0
\end{equation}
to be the local step sizes for $i = 0,..., n-1$.
Now we search for approximate values $U_i = U(t_i) \approx u(t_i) = u_i$.
By integrating both sides of \ref{eq:drvtv} over $t$ we find
\begin{equation}
  u(t_{i+1}) = u(t_i) + \int_{t_i}^{t_{i+1}} F(t,u(t)) \mathrm{d}t \qquad \text{for } i = 0,...,n-1
\end{equation}
Different methods are distinguished, depending on how the integral is solved. \emph{One-step methods} take into account only the previous value, hence calculating $U_{i+1}$ from $U_i$. \emph{Multi-step methods} use more than one previous values and \emph{extrapolation algorithms} use the Richardson approximation to improve results from integration by trapezoidal or rectangular integration methods.
In the following we want to focus on the one-step methods, especially the Runge-Kutta methods, as this is the most common one in usage.

One very simple approach is the Euler-Cauchy method, where $U_{i+1}$ is approximated just as a linear continuation from $U_i$. As this method fails even for very simple functions (e.g. think of a quadratic function which very fast will diverge from that tangential approximation), this method was improved by taking into account the function's further derivatives (tangents).
% HIER EVTL NEN BILD EINFÜGEN

\subsection{Runge-Kutta Methods}

A Runge-Kutta method (RK method) in general can be written as
\begin{align}
U_{n+1} &= U_n + \Delta t_n \sum_{i=1}^s b_i k_i(t_n, U_n, \Delta t_n) \\
&\text{where} \quad k_i(t, U, \Delta t) = F(t+c_i\Delta t, U+ \Delta t\sum_{j=0}^{i} a_{ij} k_j(t,U,\Delta t))
\end{align}
with the set of characteristic coefficients $(a_{ij}, b_i, c_i)$ for $i = 1,...,s$ where $s$ denotes the so-called \emph{stage} of the algorithm.
It estimates a function's next point by evaluating the weighted average of various derivates at pieces of the next time step.
The most common variant of doing so is using half time steps and therefor getting four helping tangents, the average value of which then acts as the linear continuations increase. Hence, this version is referred to as the \emph{Fourth Order RK method, RK4} or simply \emph{Classical RK method} and will be treated in more detail below.

\paragraph{The Butcher Table}
% Butcher table
The characterisistic coefficients $c_i, b_i$ and $a_{ij}$ are ususally written in the \emph{Butcher table}, i.e.
\begin{equation}
  \begin{pmatrix}
    c_i & a_{ij} \\
    & b_i^T
  \end{pmatrix}
  =
  \begin{pmatrix}
    0 &&&&\\
    c_2 & a_{21} & a_{22} & \dots & \\
    c_3 & a_{31} & a_{32} && \\
    \vdots & \vdots & &\ddots &\\
    c_s & a_{s1} & a_{s2} & \dots & a_{s, s-1} \\
    & b_1 & b_2 & \dots & b_{s-1} & b_s
  \end{pmatrix}
\end{equation}





\paragraph{Explicit and Implicit RK methods}
A RK method can be either explicit or implicit, depending on how many coefficients $a_{ij}$ are taken into account for the calculation of $k_i$.
If $a_{ij}$ are set to zero for $i \leq j$, the internal stages $k_{n1},..., k_{ns}$ can be be computed directly, i.e. explicitly.
With respect to the Butcher table, this means that explicit methods only have the lower diagonal elements of the the $a_{ij}$ nonzero, i.e. a triangular matrix.
On the other hand, if all $a_{ij}$ are used and hence the matrix is fully occupied,
then the calculation of $k_i$ ends up as a nonlinear system and is therefor called implicit. Implicit methods tend to have a higher stability, as more terms are used. Their application is required for some partial differential equations, especially for stiff problems. % , where a very high stability is reuired, one may consider using an implicit RK method
% QUELLE: 1989 BookTheNumericalSolutionOfDifferen


\paragraph{Estimated Error}
From the stage number $s$ one can estimate the magnitude of error, e.g. for $s \leq 4$ the local error is of order $s +1$.
For larger stage numbers this linear relation will flatten so that the order of error grows slower than the stage number.
% QUELLE: Pertsch-Skript


\paragraph{Example Code for the 4-stage RK Method}
% sehr schöne Darstellung des Verfahrens: http://www.kohorst-lemgo.de/modell/runge.htm

....
(Derivation of how RK4 has error of order t5)
.....


The RK4 method usually comes with the Butcher table
\begin{equation}
  \begin{pmatrix}
    0 &&&\\
    1/2 & 1/2 & & \\
    1/2 & 0 & 1/2 & \\
    1 & 0 & 0 & 1  \\
    & 1/6 & 1/3 & 1/3 & 1/6
  \end{pmatrix}
\end{equation}
and hence the various tangents are
\begin{align}
  k_1 &= F(t, U) \\
  k_2 &= F(t + \frac{1}{2} \Delta t, U + \frac{1}{2} \Delta t k_1) \\
  k_3 &= F(t + \frac{1}{2} \Delta t, U + \frac{1}{2} \Delta t k_2 ) \\
  k_4 &= F(t + \Delta t, U + \Delta t k_3),
\end{align}
which by being weighted yield the estimated function value
\begin{equation}
  U_{n+1} = U_n + \Delta t \left(\frac{1}{6} k_1 + \frac{1}{3} k_2 + \frac{1}{3} k_3 + \frac{1}{6} k_4 \right)~.
\end{equation}
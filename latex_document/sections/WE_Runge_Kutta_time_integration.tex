\section{Runge-Kutta time-integration}

After the former chapters treated numerical method to calculate the (spatial) derivative of a given function, we now want to devote to the temporal integration in order to deal with the initial value problem (IVP) of an ordinary differential equation (ODE). I.e. we want to find the function $u(t)$, and all we know is its derivative
\begin{equation}
  \frac{\mathrm{d} u}{\mathrm{d} t} = F(u)
  \label{eq:drvtv}
\end{equation}
and the initial value $u(t = 0) = u_0$.
We assume $F(u)$ to be continuous, especially Lipschitz continuous, to make sure that there exists exactly one solution for the IVP.
% QUELLE : 2011_Book_Numerik-Algorithmen
\subsection{General Principle of Numerical Integration Methods}
Similarly to the spatial differential methods, the first step is to discretize the respective interval. Generally, this discretization does not have to be equidistant, so we define
\begin{equation}
  \Delta t_i = t_{i+1} - t_i >0
\end{equation}
to be the local step sizes for $i = 0,..., n-1$.
Now we search for approximate values $U_i = U(t_i) \approx u(t_i) = u_i$.
By integrating both sides of \ref{eq:drvtv} over $t$ we find
\begin{equation}
  u(t_{i+1}) = u(t_i) + \int_{t_i}^{t_{i+1}} F(t,u(t)) \mathrm{d}t \qquad \text{for } i = 0,...,n-1
\end{equation}
Different methods are distinguished, depending on how the integral is solved. \emph{One-step methods} take into account only the previous value, hence calculating $U_{i+1}$ from $U_i$. \emph{Multi-step methods} use more than one previous values and \emph{extrapolation algorithms} use the Richardson approximation to improve results from integration by trapezoidal or rectangular integration methods.
In the following we want to focus on the one-step methods, especially the Runge-Kutta methods, as this is the most common one in usage.

One very simple approach is the Euler-Cauchy method, where $U_{i+1}$ is approximated just as a linear continuation from $U_i$. As this method fails even for very simple functions (e.g. think of a quadratic function which very fast will diverge from that tangential approximation), this method was improved by taking into account the function's further derivatives (tangents).
% HIER EVTL NEN BILD EINFÜGEN

\subsection{Runge-Kutta Methods}

A Runge-Kutta method (RK method) in general can be written as
\begin{align}
U_{n+1} &= U_n + \Delta t_n \sum_{i=1}^s b_i k_i(t_n, U_n, \Delta t_n) \\
&\text{where} \quad k_i(t, U, \Delta t) = F(t+c_i\Delta t, U+ \Delta t\sum_{j=0}^{i} a_{ij} k_j(t,U,\Delta t))
\end{align}
with the set of characteristic coefficients $(a_{ij}, b_i, c_i)$ for $i = 1,...,s$ where $s$ denotes the so-called \emph{stage} of the algorithm.
It estimates a function's next point by evaluating the weighted average of various derivates at pieces of the next time step.
The most common variant of doing so is using half time steps and therefor getting four helping tangents, the average value of which then acts as the linear continuations increase. Hence, this version is referred to as the \emph{Fourth Order RK method, RK4} or simply \emph{Classical RK method} and will be treated in more detail below.

\paragraph{The Butcher Table}
% Butcher table
The characterisistic coefficients $c_i, b_i$ and $a_{ij}$ are ususally written in the \emph{Butcher table}, i.e.
\begin{equation}
  \begin{pmatrix}
    c_i & a_{ij} \\
    & b_i^T
  \end{pmatrix}
  =
  \begin{pmatrix}
    0 &&&&\\
    c_2 & a_{21} & a_{22} & \dots & \\
    c_3 & a_{31} & a_{32} && \\
    \vdots & \vdots & &\ddots &\\
    c_s & a_{s1} & a_{s2} & \dots & a_{s, s-1} \\
    & b_1 & b_2 & \dots & b_{s-1} & b_s
  \end{pmatrix}
\end{equation}





\paragraph{Explicit and Implicit RK methods}
A RK method can be either explicit or implicit, depending on how many coefficients $a_{ij}$ are taken into account for the calculation of $k_i$.
If $a_{ij}$ are set to zero for $i \leq j$, the internal stages $k_{n1},..., k_{ns}$ can be be computed directly, i.e. explicitly.
With respect to the Butcher table, this means that explicit methods only have the lower diagonal elements of the the $a_{ij}$ nonzero, i.e. a triangular matrix.
On the other hand, if all $a_{ij}$ are used and hence the matrix is fully occupied,
then the calculation of $k_i$ ends up as a nonlinear system and is therefor called implicit. Implicit methods tend to have a higher stability, as more terms are used. Their application is required for some partial differential equations, especially for stiff problems. % , where a very high stability is reuired, one may consider using an implicit RK method
% QUELLE: 1989 BookTheNumericalSolutionOfDifferen


\paragraph{Estimated Error}
From the stage number $s$ one can estimate the magnitude of error, e.g. for $s \leq 4$ the local error is of order $s +1$.
For larger stage numbers this linear relation will flatten so that the order of error grows slower than the stage number.
% QUELLE: Pertsch-Skript


\paragraph{The classical RK method: RK4}
% sehr schöne Darstellung des Verfahrens: http://www.kohorst-lemgo.de/modell/runge.htm

....
(Derivation of how RK4 has error of order t5)
.....


The RK4 method usually comes with the Butcher table
\begin{equation}
  \begin{pmatrix}
    0 &&&\\
    1/2 & 1/2 & & \\
    1/2 & 0 & 1/2 & \\
    1 & 0 & 0 & 1  \\
    & 1/6 & 1/3 & 1/3 & 1/6
  \end{pmatrix}
\end{equation}
and hence the various tangents are
\begin{align}
  k_1 &= F(t, U) \\
  k_2 &= F(t + \frac{1}{2} \Delta t, U + \frac{1}{2} \Delta t k_1) \\
  k_3 &= F(t + \frac{1}{2} \Delta t, U + \frac{1}{2} \Delta t k_2 ) \\
  k_4 &= F(t + \Delta t, U + \Delta t k_3),
\end{align}
which by being weighted yield the estimated function value
\begin{equation}
  U_{n+1} = U_n + \Delta t \left(\frac{1}{6} k_1 + \frac{1}{3} k_2 + \frac{1}{3} k_3 + \frac{1}{6} k_4 \right)~.
\end{equation}
\paragraph{Example: Solving the Harmonic Oscillator Equation using RK4}
Given is the Hamiltonian for the harmonic oscillator
\begin{equation}
  H(q,p) = \frac{1}{2} \left(\frac{p^2}{m} + q^2m \right)
\end{equation}
where $q = \omega x$ and $p = m \dot{x}$, as well as the initial values $q_0$ and $p_0$. The Hamiltonian yields the equations of movement, i.e. the time-derivatives of the generalized coordinates $q$ and $p$
\begin{align}
  \dot{q} = \frac{\partial H}{\partial q} = \frac{p}{m} \qquad \qquad
  \dot{p} = \frac{\partial H}{\partial p} = -q m
\end{align}
so that we can write our system of ODEs in simple vector form, introducing $u = \{q,p\}$:
\begin{equation}
  \tfrac{\mathrm{d}}{\mathrm{d}t} u =
  \begin{bmatrix} \frac{p}{m} \\ - q m
  \end{bmatrix}.
  \label{eq:dudt}
\end{equation}
In the following we present a possible implementation to solve this initial value problem, given by \ref{eq:dudt} and $u_0 = {q_0, p_0}$, using the RK4 method.
The implementation is divided in two functions: \lstinline{F(u,t)} and \lstinline{rk4(u0, deltat, T, F)}. The former returns the differentiation of $u$, i.e. \ref{eq:dudt}. The latter is the actual approximation of function values. Here, above's algorithm is straight-forward implemented.
\begin{lstlisting}
def F(u,t):
  dqdt = u[1]/m
  dpdt = -u[0]*m
  return np.array([dqdt,dpdt])

def rk4(u0,deltat,T,F):
  u = np.zeros([n,2])
  u[0,0] = q0
  u[0,1] = p0
  t=1
  for i in range(0,n-1):
    k1 = F(u[i,:], t)
    k2 = F(u[i,:] + 0.5*deltat* k1,t + 0.5*deltat)
    k3 = F(u[i,:] + 0.5*deltat* k2,t + 0.5*deltat)
    k4 = F(u[i,:] + deltat* k3,t + deltat)
    u[i+1,:] = u[i,:] + deltat*(1/6*k1 + 1/3*k2 +1/3*k3 + 1/6*k4)
  return u

u_rk4 = rk4(u0,deltat,T,F)
\end{lstlisting}
It shall be noted, first, that \lstinline{u} is an array of 2 columns (one for values of $q$ and $p$ respectively) and as long as the number of time steps. Second, there occurs a time variable \lstinline{t} in the implemented function \lstinline{F(u,t)} which is actually unnecessary for the harmonic oscillator. Nevertheless it is written there, just to show, that there could possibly be a time dependence (e.g. for the heat equation this is the case).





